{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "card_fraud.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZhq0K5e89XT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.utils import to_categorical\n",
        "import collections\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-eCbe7l89XW",
        "colab_type": "text"
      },
      "source": [
        "### CSV 파일에서 데이터를 원하는 형태로 가공하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykxCFh8z89XX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "550a3f7c-37e5-4b62-bc55-e09ed1bcc923"
      },
      "source": [
        "# csv 파일 읽기\n",
        "df = pd.read_csv(\"creditcard.csv\")\n",
        "\n",
        "# 492 fraudulent transactions, 284,315 normal transactions.\n",
        "# 0.172% of transactions were fraud.\n",
        "print(df.values.shape[0])\n",
        "print(df.values.shape)\n",
        "print(df.Class.value_counts())\n",
        "print(df.Time.values)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "284807\n",
            "(284807, 31)\n",
            "0    284315\n",
            "1       492\n",
            "Name: Class, dtype: int64\n",
            "[0.00000e+00 0.00000e+00 1.00000e+00 ... 1.72788e+05 1.72788e+05\n",
            " 1.72792e+05]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_eu1BiK89XZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "outputId": "59baae39-78f0-4d4d-fb5c-3a51e52fd8c1"
      },
      "source": [
        "df[:10]"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.359807</td>\n",
              "      <td>-0.072781</td>\n",
              "      <td>2.536347</td>\n",
              "      <td>1.378155</td>\n",
              "      <td>-0.338321</td>\n",
              "      <td>0.462388</td>\n",
              "      <td>0.239599</td>\n",
              "      <td>0.098698</td>\n",
              "      <td>0.363787</td>\n",
              "      <td>0.090794</td>\n",
              "      <td>-0.551600</td>\n",
              "      <td>-0.617801</td>\n",
              "      <td>-0.991390</td>\n",
              "      <td>-0.311169</td>\n",
              "      <td>1.468177</td>\n",
              "      <td>-0.470401</td>\n",
              "      <td>0.207971</td>\n",
              "      <td>0.025791</td>\n",
              "      <td>0.403993</td>\n",
              "      <td>0.251412</td>\n",
              "      <td>-0.018307</td>\n",
              "      <td>0.277838</td>\n",
              "      <td>-0.110474</td>\n",
              "      <td>0.066928</td>\n",
              "      <td>0.128539</td>\n",
              "      <td>-0.189115</td>\n",
              "      <td>0.133558</td>\n",
              "      <td>-0.021053</td>\n",
              "      <td>149.62</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.191857</td>\n",
              "      <td>0.266151</td>\n",
              "      <td>0.166480</td>\n",
              "      <td>0.448154</td>\n",
              "      <td>0.060018</td>\n",
              "      <td>-0.082361</td>\n",
              "      <td>-0.078803</td>\n",
              "      <td>0.085102</td>\n",
              "      <td>-0.255425</td>\n",
              "      <td>-0.166974</td>\n",
              "      <td>1.612727</td>\n",
              "      <td>1.065235</td>\n",
              "      <td>0.489095</td>\n",
              "      <td>-0.143772</td>\n",
              "      <td>0.635558</td>\n",
              "      <td>0.463917</td>\n",
              "      <td>-0.114805</td>\n",
              "      <td>-0.183361</td>\n",
              "      <td>-0.145783</td>\n",
              "      <td>-0.069083</td>\n",
              "      <td>-0.225775</td>\n",
              "      <td>-0.638672</td>\n",
              "      <td>0.101288</td>\n",
              "      <td>-0.339846</td>\n",
              "      <td>0.167170</td>\n",
              "      <td>0.125895</td>\n",
              "      <td>-0.008983</td>\n",
              "      <td>0.014724</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.358354</td>\n",
              "      <td>-1.340163</td>\n",
              "      <td>1.773209</td>\n",
              "      <td>0.379780</td>\n",
              "      <td>-0.503198</td>\n",
              "      <td>1.800499</td>\n",
              "      <td>0.791461</td>\n",
              "      <td>0.247676</td>\n",
              "      <td>-1.514654</td>\n",
              "      <td>0.207643</td>\n",
              "      <td>0.624501</td>\n",
              "      <td>0.066084</td>\n",
              "      <td>0.717293</td>\n",
              "      <td>-0.165946</td>\n",
              "      <td>2.345865</td>\n",
              "      <td>-2.890083</td>\n",
              "      <td>1.109969</td>\n",
              "      <td>-0.121359</td>\n",
              "      <td>-2.261857</td>\n",
              "      <td>0.524980</td>\n",
              "      <td>0.247998</td>\n",
              "      <td>0.771679</td>\n",
              "      <td>0.909412</td>\n",
              "      <td>-0.689281</td>\n",
              "      <td>-0.327642</td>\n",
              "      <td>-0.139097</td>\n",
              "      <td>-0.055353</td>\n",
              "      <td>-0.059752</td>\n",
              "      <td>378.66</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.966272</td>\n",
              "      <td>-0.185226</td>\n",
              "      <td>1.792993</td>\n",
              "      <td>-0.863291</td>\n",
              "      <td>-0.010309</td>\n",
              "      <td>1.247203</td>\n",
              "      <td>0.237609</td>\n",
              "      <td>0.377436</td>\n",
              "      <td>-1.387024</td>\n",
              "      <td>-0.054952</td>\n",
              "      <td>-0.226487</td>\n",
              "      <td>0.178228</td>\n",
              "      <td>0.507757</td>\n",
              "      <td>-0.287924</td>\n",
              "      <td>-0.631418</td>\n",
              "      <td>-1.059647</td>\n",
              "      <td>-0.684093</td>\n",
              "      <td>1.965775</td>\n",
              "      <td>-1.232622</td>\n",
              "      <td>-0.208038</td>\n",
              "      <td>-0.108300</td>\n",
              "      <td>0.005274</td>\n",
              "      <td>-0.190321</td>\n",
              "      <td>-1.175575</td>\n",
              "      <td>0.647376</td>\n",
              "      <td>-0.221929</td>\n",
              "      <td>0.062723</td>\n",
              "      <td>0.061458</td>\n",
              "      <td>123.50</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.0</td>\n",
              "      <td>-1.158233</td>\n",
              "      <td>0.877737</td>\n",
              "      <td>1.548718</td>\n",
              "      <td>0.403034</td>\n",
              "      <td>-0.407193</td>\n",
              "      <td>0.095921</td>\n",
              "      <td>0.592941</td>\n",
              "      <td>-0.270533</td>\n",
              "      <td>0.817739</td>\n",
              "      <td>0.753074</td>\n",
              "      <td>-0.822843</td>\n",
              "      <td>0.538196</td>\n",
              "      <td>1.345852</td>\n",
              "      <td>-1.119670</td>\n",
              "      <td>0.175121</td>\n",
              "      <td>-0.451449</td>\n",
              "      <td>-0.237033</td>\n",
              "      <td>-0.038195</td>\n",
              "      <td>0.803487</td>\n",
              "      <td>0.408542</td>\n",
              "      <td>-0.009431</td>\n",
              "      <td>0.798278</td>\n",
              "      <td>-0.137458</td>\n",
              "      <td>0.141267</td>\n",
              "      <td>-0.206010</td>\n",
              "      <td>0.502292</td>\n",
              "      <td>0.219422</td>\n",
              "      <td>0.215153</td>\n",
              "      <td>69.99</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2.0</td>\n",
              "      <td>-0.425966</td>\n",
              "      <td>0.960523</td>\n",
              "      <td>1.141109</td>\n",
              "      <td>-0.168252</td>\n",
              "      <td>0.420987</td>\n",
              "      <td>-0.029728</td>\n",
              "      <td>0.476201</td>\n",
              "      <td>0.260314</td>\n",
              "      <td>-0.568671</td>\n",
              "      <td>-0.371407</td>\n",
              "      <td>1.341262</td>\n",
              "      <td>0.359894</td>\n",
              "      <td>-0.358091</td>\n",
              "      <td>-0.137134</td>\n",
              "      <td>0.517617</td>\n",
              "      <td>0.401726</td>\n",
              "      <td>-0.058133</td>\n",
              "      <td>0.068653</td>\n",
              "      <td>-0.033194</td>\n",
              "      <td>0.084968</td>\n",
              "      <td>-0.208254</td>\n",
              "      <td>-0.559825</td>\n",
              "      <td>-0.026398</td>\n",
              "      <td>-0.371427</td>\n",
              "      <td>-0.232794</td>\n",
              "      <td>0.105915</td>\n",
              "      <td>0.253844</td>\n",
              "      <td>0.081080</td>\n",
              "      <td>3.67</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>4.0</td>\n",
              "      <td>1.229658</td>\n",
              "      <td>0.141004</td>\n",
              "      <td>0.045371</td>\n",
              "      <td>1.202613</td>\n",
              "      <td>0.191881</td>\n",
              "      <td>0.272708</td>\n",
              "      <td>-0.005159</td>\n",
              "      <td>0.081213</td>\n",
              "      <td>0.464960</td>\n",
              "      <td>-0.099254</td>\n",
              "      <td>-1.416907</td>\n",
              "      <td>-0.153826</td>\n",
              "      <td>-0.751063</td>\n",
              "      <td>0.167372</td>\n",
              "      <td>0.050144</td>\n",
              "      <td>-0.443587</td>\n",
              "      <td>0.002821</td>\n",
              "      <td>-0.611987</td>\n",
              "      <td>-0.045575</td>\n",
              "      <td>-0.219633</td>\n",
              "      <td>-0.167716</td>\n",
              "      <td>-0.270710</td>\n",
              "      <td>-0.154104</td>\n",
              "      <td>-0.780055</td>\n",
              "      <td>0.750137</td>\n",
              "      <td>-0.257237</td>\n",
              "      <td>0.034507</td>\n",
              "      <td>0.005168</td>\n",
              "      <td>4.99</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7.0</td>\n",
              "      <td>-0.644269</td>\n",
              "      <td>1.417964</td>\n",
              "      <td>1.074380</td>\n",
              "      <td>-0.492199</td>\n",
              "      <td>0.948934</td>\n",
              "      <td>0.428118</td>\n",
              "      <td>1.120631</td>\n",
              "      <td>-3.807864</td>\n",
              "      <td>0.615375</td>\n",
              "      <td>1.249376</td>\n",
              "      <td>-0.619468</td>\n",
              "      <td>0.291474</td>\n",
              "      <td>1.757964</td>\n",
              "      <td>-1.323865</td>\n",
              "      <td>0.686133</td>\n",
              "      <td>-0.076127</td>\n",
              "      <td>-1.222127</td>\n",
              "      <td>-0.358222</td>\n",
              "      <td>0.324505</td>\n",
              "      <td>-0.156742</td>\n",
              "      <td>1.943465</td>\n",
              "      <td>-1.015455</td>\n",
              "      <td>0.057504</td>\n",
              "      <td>-0.649709</td>\n",
              "      <td>-0.415267</td>\n",
              "      <td>-0.051634</td>\n",
              "      <td>-1.206921</td>\n",
              "      <td>-1.085339</td>\n",
              "      <td>40.80</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>7.0</td>\n",
              "      <td>-0.894286</td>\n",
              "      <td>0.286157</td>\n",
              "      <td>-0.113192</td>\n",
              "      <td>-0.271526</td>\n",
              "      <td>2.669599</td>\n",
              "      <td>3.721818</td>\n",
              "      <td>0.370145</td>\n",
              "      <td>0.851084</td>\n",
              "      <td>-0.392048</td>\n",
              "      <td>-0.410430</td>\n",
              "      <td>-0.705117</td>\n",
              "      <td>-0.110452</td>\n",
              "      <td>-0.286254</td>\n",
              "      <td>0.074355</td>\n",
              "      <td>-0.328783</td>\n",
              "      <td>-0.210077</td>\n",
              "      <td>-0.499768</td>\n",
              "      <td>0.118765</td>\n",
              "      <td>0.570328</td>\n",
              "      <td>0.052736</td>\n",
              "      <td>-0.073425</td>\n",
              "      <td>-0.268092</td>\n",
              "      <td>-0.204233</td>\n",
              "      <td>1.011592</td>\n",
              "      <td>0.373205</td>\n",
              "      <td>-0.384157</td>\n",
              "      <td>0.011747</td>\n",
              "      <td>0.142404</td>\n",
              "      <td>93.20</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9.0</td>\n",
              "      <td>-0.338262</td>\n",
              "      <td>1.119593</td>\n",
              "      <td>1.044367</td>\n",
              "      <td>-0.222187</td>\n",
              "      <td>0.499361</td>\n",
              "      <td>-0.246761</td>\n",
              "      <td>0.651583</td>\n",
              "      <td>0.069539</td>\n",
              "      <td>-0.736727</td>\n",
              "      <td>-0.366846</td>\n",
              "      <td>1.017614</td>\n",
              "      <td>0.836390</td>\n",
              "      <td>1.006844</td>\n",
              "      <td>-0.443523</td>\n",
              "      <td>0.150219</td>\n",
              "      <td>0.739453</td>\n",
              "      <td>-0.540980</td>\n",
              "      <td>0.476677</td>\n",
              "      <td>0.451773</td>\n",
              "      <td>0.203711</td>\n",
              "      <td>-0.246914</td>\n",
              "      <td>-0.633753</td>\n",
              "      <td>-0.120794</td>\n",
              "      <td>-0.385050</td>\n",
              "      <td>-0.069733</td>\n",
              "      <td>0.094199</td>\n",
              "      <td>0.246219</td>\n",
              "      <td>0.083076</td>\n",
              "      <td>3.68</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
              "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
              "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
              "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
              "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
              "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
              "5   2.0 -0.425966  0.960523  1.141109 -0.168252  0.420987 -0.029728  0.476201   \n",
              "6   4.0  1.229658  0.141004  0.045371  1.202613  0.191881  0.272708 -0.005159   \n",
              "7   7.0 -0.644269  1.417964  1.074380 -0.492199  0.948934  0.428118  1.120631   \n",
              "8   7.0 -0.894286  0.286157 -0.113192 -0.271526  2.669599  3.721818  0.370145   \n",
              "9   9.0 -0.338262  1.119593  1.044367 -0.222187  0.499361 -0.246761  0.651583   \n",
              "\n",
              "         V8        V9       V10       V11       V12       V13       V14  \\\n",
              "0  0.098698  0.363787  0.090794 -0.551600 -0.617801 -0.991390 -0.311169   \n",
              "1  0.085102 -0.255425 -0.166974  1.612727  1.065235  0.489095 -0.143772   \n",
              "2  0.247676 -1.514654  0.207643  0.624501  0.066084  0.717293 -0.165946   \n",
              "3  0.377436 -1.387024 -0.054952 -0.226487  0.178228  0.507757 -0.287924   \n",
              "4 -0.270533  0.817739  0.753074 -0.822843  0.538196  1.345852 -1.119670   \n",
              "5  0.260314 -0.568671 -0.371407  1.341262  0.359894 -0.358091 -0.137134   \n",
              "6  0.081213  0.464960 -0.099254 -1.416907 -0.153826 -0.751063  0.167372   \n",
              "7 -3.807864  0.615375  1.249376 -0.619468  0.291474  1.757964 -1.323865   \n",
              "8  0.851084 -0.392048 -0.410430 -0.705117 -0.110452 -0.286254  0.074355   \n",
              "9  0.069539 -0.736727 -0.366846  1.017614  0.836390  1.006844 -0.443523   \n",
              "\n",
              "        V15       V16       V17       V18       V19       V20       V21  \\\n",
              "0  1.468177 -0.470401  0.207971  0.025791  0.403993  0.251412 -0.018307   \n",
              "1  0.635558  0.463917 -0.114805 -0.183361 -0.145783 -0.069083 -0.225775   \n",
              "2  2.345865 -2.890083  1.109969 -0.121359 -2.261857  0.524980  0.247998   \n",
              "3 -0.631418 -1.059647 -0.684093  1.965775 -1.232622 -0.208038 -0.108300   \n",
              "4  0.175121 -0.451449 -0.237033 -0.038195  0.803487  0.408542 -0.009431   \n",
              "5  0.517617  0.401726 -0.058133  0.068653 -0.033194  0.084968 -0.208254   \n",
              "6  0.050144 -0.443587  0.002821 -0.611987 -0.045575 -0.219633 -0.167716   \n",
              "7  0.686133 -0.076127 -1.222127 -0.358222  0.324505 -0.156742  1.943465   \n",
              "8 -0.328783 -0.210077 -0.499768  0.118765  0.570328  0.052736 -0.073425   \n",
              "9  0.150219  0.739453 -0.540980  0.476677  0.451773  0.203711 -0.246914   \n",
              "\n",
              "        V22       V23       V24       V25       V26       V27       V28  \\\n",
              "0  0.277838 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053   \n",
              "1 -0.638672  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724   \n",
              "2  0.771679  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752   \n",
              "3  0.005274 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458   \n",
              "4  0.798278 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   \n",
              "5 -0.559825 -0.026398 -0.371427 -0.232794  0.105915  0.253844  0.081080   \n",
              "6 -0.270710 -0.154104 -0.780055  0.750137 -0.257237  0.034507  0.005168   \n",
              "7 -1.015455  0.057504 -0.649709 -0.415267 -0.051634 -1.206921 -1.085339   \n",
              "8 -0.268092 -0.204233  1.011592  0.373205 -0.384157  0.011747  0.142404   \n",
              "9 -0.633753 -0.120794 -0.385050 -0.069733  0.094199  0.246219  0.083076   \n",
              "\n",
              "   Amount  Class  \n",
              "0  149.62      0  \n",
              "1    2.69      0  \n",
              "2  378.66      0  \n",
              "3  123.50      0  \n",
              "4   69.99      0  \n",
              "5    3.67      0  \n",
              "6    4.99      0  \n",
              "7   40.80      0  \n",
              "8   93.20      0  \n",
              "9    3.68      0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kON_TFhT89Xf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "294549a5-fbd2-49e1-dd55-89f1d4169852"
      },
      "source": [
        "print(type(df.Class))\n",
        "print(type(df['Class']))\n",
        "aa = df[['Class']]\n",
        "bb = df[['Class', 'Amount']]\n",
        "print(type(aa))\n",
        "print(type(bb))\n",
        "print(aa.values.shape)\n",
        "print(bb.values.shape)\n",
        "len(df.Time.values)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.series.Series'>\n",
            "<class 'pandas.core.series.Series'>\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "(284807, 1)\n",
            "(284807, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "284807"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9beNxpA-89Xi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "b0ca6778-a96a-4fe2-f1ac-8018677786ef"
      },
      "source": [
        "df[df.Class == 1].count()"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Time      492\n",
              "V1        492\n",
              "V2        492\n",
              "V3        492\n",
              "V4        492\n",
              "V5        492\n",
              "V6        492\n",
              "V7        492\n",
              "V8        492\n",
              "V9        492\n",
              "V10       492\n",
              "V11       492\n",
              "V12       492\n",
              "V13       492\n",
              "V14       492\n",
              "V15       492\n",
              "V16       492\n",
              "V17       492\n",
              "V18       492\n",
              "V19       492\n",
              "V20       492\n",
              "V21       492\n",
              "V22       492\n",
              "V23       492\n",
              "V24       492\n",
              "V25       492\n",
              "V26       492\n",
              "V27       492\n",
              "V28       492\n",
              "Amount    492\n",
              "Class     492\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2J9p4Pa89Xk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "96f88f93-aef0-48ff-9282-9a7174a6934e"
      },
      "source": [
        "df.V2.value_counts()"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              " 0.166975    77\n",
              "-0.326668    77\n",
              " 0.089735    62\n",
              "-0.606529    60\n",
              "-0.146975    53\n",
              " 0.393051    48\n",
              " 0.389198    45\n",
              " 1.036663    40\n",
              " 0.331464    39\n",
              " 0.447474    36\n",
              "-0.033577    36\n",
              "-0.207740    35\n",
              "-0.367489    30\n",
              " 0.033896    28\n",
              " 0.075028    27\n",
              "-0.715029    27\n",
              "-1.197678    26\n",
              " 0.008527    25\n",
              "-0.699902    25\n",
              "-0.023838    24\n",
              " 0.185411    24\n",
              " 1.265621    24\n",
              "-1.213639    23\n",
              "-0.528597    23\n",
              " 0.273058    21\n",
              "-0.300112    21\n",
              "-0.434463    21\n",
              " 0.457969    21\n",
              "-0.364010    20\n",
              "-0.958269    19\n",
              "             ..\n",
              "-0.895296     1\n",
              " 0.133219     1\n",
              " 0.791998     1\n",
              " 2.623280     1\n",
              "-0.704504     1\n",
              " 0.133674     1\n",
              "-0.031065     1\n",
              "-7.852369     1\n",
              "-0.365261     1\n",
              "-0.853673     1\n",
              " 0.140413     1\n",
              " 1.502102     1\n",
              " 0.829049     1\n",
              " 1.050976     1\n",
              "-0.092252     1\n",
              "-0.385766     1\n",
              "-0.427667     1\n",
              " 1.194539     1\n",
              " 1.344977     1\n",
              " 1.901399     1\n",
              "-0.265412     1\n",
              " 1.348840     1\n",
              " 0.385590     1\n",
              "-0.642216     1\n",
              " 5.099359     1\n",
              " 0.520114     1\n",
              "-0.419199     1\n",
              "-0.303794     1\n",
              " 0.167497     1\n",
              "-0.166213     1\n",
              "Name: V2, Length: 275663, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Iz0ax6YJVQ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a new feature for normal (non-fraudulent) transactions.\n",
        "# Normal column has inverse value of Class column \n",
        "df.loc[df.Class == 0, 'Normal'] = 1\n",
        "df.loc[df.Class == 1, 'Normal'] = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-lbCeEg89Xm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ee64e5a4-96e0-4eab-daac-87c362d1d47e"
      },
      "source": [
        "# Rename 'Class' to 'Fraud'\n",
        "# Class -> Fraud 로 명 변환\n",
        "df = df.rename(columns = {'Class' : 'Fraud'})\n",
        "\n",
        "\n",
        "# max column 수 설정\n",
        "pd.set_option('display.max_columns',  101)\n",
        "# print(df.head())\n",
        "\n",
        "# Create dataframes of only Fraud and Normal transactions.\n",
        "Fraud = df[df.Fraud == 1]\n",
        "Normal = df[df.Normal == 1]\n",
        "print('Fraud  : ', len(Fraud))\n",
        "print('Normal : ', len(Normal))"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fraud  :  492\n",
            "Normal :  284315\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CGI7tY9SGua",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "a7e34f37-72fa-4bf1-ea5c-fe0c9db2f9d3"
      },
      "source": [
        "print(df.Normal.value_counts())\n",
        "df.Fraud.value_counts()"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0    284315\n",
            "0.0       492\n",
            "Name: Normal, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    284315\n",
              "1       492\n",
              "Name: Fraud, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56fdUwJ0N0lg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "023c0e7f-e30f-45f8-f8e5-284a5aadf94c"
      },
      "source": [
        "for_train[:5]"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Fraud</th>\n",
              "      <th>Normal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>100902.0</td>\n",
              "      <td>1.908902</td>\n",
              "      <td>-0.669120</td>\n",
              "      <td>-0.114980</td>\n",
              "      <td>0.264529</td>\n",
              "      <td>-0.595843</td>\n",
              "      <td>0.603159</td>\n",
              "      <td>-1.203876</td>\n",
              "      <td>0.300718</td>\n",
              "      <td>2.606065</td>\n",
              "      <td>-0.213019</td>\n",
              "      <td>1.189008</td>\n",
              "      <td>-2.281706</td>\n",
              "      <td>0.650900</td>\n",
              "      <td>1.514962</td>\n",
              "      <td>-0.411332</td>\n",
              "      <td>0.990916</td>\n",
              "      <td>-0.216454</td>\n",
              "      <td>1.041724</td>\n",
              "      <td>-0.114399</td>\n",
              "      <td>-0.187281</td>\n",
              "      <td>-0.090820</td>\n",
              "      <td>-0.048897</td>\n",
              "      <td>0.293753</td>\n",
              "      <td>0.154149</td>\n",
              "      <td>-0.638667</td>\n",
              "      <td>0.423484</td>\n",
              "      <td>-0.058530</td>\n",
              "      <td>-0.049864</td>\n",
              "      <td>39.00</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>135812.0</td>\n",
              "      <td>-0.008638</td>\n",
              "      <td>0.131883</td>\n",
              "      <td>0.635344</td>\n",
              "      <td>-0.399317</td>\n",
              "      <td>0.577579</td>\n",
              "      <td>0.332326</td>\n",
              "      <td>0.527800</td>\n",
              "      <td>-0.133700</td>\n",
              "      <td>0.232359</td>\n",
              "      <td>0.004998</td>\n",
              "      <td>-0.003635</td>\n",
              "      <td>0.619642</td>\n",
              "      <td>0.684039</td>\n",
              "      <td>-0.323506</td>\n",
              "      <td>-0.354839</td>\n",
              "      <td>0.616552</td>\n",
              "      <td>-1.259022</td>\n",
              "      <td>0.500995</td>\n",
              "      <td>0.814613</td>\n",
              "      <td>0.048418</td>\n",
              "      <td>-0.179809</td>\n",
              "      <td>-0.340650</td>\n",
              "      <td>0.142739</td>\n",
              "      <td>-1.047634</td>\n",
              "      <td>-1.118483</td>\n",
              "      <td>-0.115849</td>\n",
              "      <td>-0.163665</td>\n",
              "      <td>-0.089465</td>\n",
              "      <td>65.91</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>159914.0</td>\n",
              "      <td>1.880701</td>\n",
              "      <td>-0.034917</td>\n",
              "      <td>-0.698435</td>\n",
              "      <td>1.484986</td>\n",
              "      <td>-0.299145</td>\n",
              "      <td>-1.002989</td>\n",
              "      <td>0.147486</td>\n",
              "      <td>-0.279035</td>\n",
              "      <td>0.469875</td>\n",
              "      <td>0.193943</td>\n",
              "      <td>-0.465595</td>\n",
              "      <td>0.708619</td>\n",
              "      <td>0.340083</td>\n",
              "      <td>0.192576</td>\n",
              "      <td>0.076515</td>\n",
              "      <td>-0.277451</td>\n",
              "      <td>-0.309101</td>\n",
              "      <td>-0.435506</td>\n",
              "      <td>-0.673627</td>\n",
              "      <td>-0.182125</td>\n",
              "      <td>0.136601</td>\n",
              "      <td>0.515065</td>\n",
              "      <td>0.126878</td>\n",
              "      <td>0.399356</td>\n",
              "      <td>0.054687</td>\n",
              "      <td>-0.622530</td>\n",
              "      <td>0.021559</td>\n",
              "      <td>-0.032091</td>\n",
              "      <td>46.47</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>161123.0</td>\n",
              "      <td>-0.026897</td>\n",
              "      <td>1.515108</td>\n",
              "      <td>-1.569530</td>\n",
              "      <td>-0.104253</td>\n",
              "      <td>1.051111</td>\n",
              "      <td>-1.782796</td>\n",
              "      <td>1.336510</td>\n",
              "      <td>-0.133662</td>\n",
              "      <td>-0.871216</td>\n",
              "      <td>-1.183370</td>\n",
              "      <td>-0.461643</td>\n",
              "      <td>0.235192</td>\n",
              "      <td>0.509155</td>\n",
              "      <td>-0.281912</td>\n",
              "      <td>0.183666</td>\n",
              "      <td>-0.410558</td>\n",
              "      <td>0.888893</td>\n",
              "      <td>0.136806</td>\n",
              "      <td>-0.002083</td>\n",
              "      <td>-0.198616</td>\n",
              "      <td>0.267003</td>\n",
              "      <td>0.795362</td>\n",
              "      <td>-0.206794</td>\n",
              "      <td>-0.059156</td>\n",
              "      <td>-0.098000</td>\n",
              "      <td>-0.149678</td>\n",
              "      <td>-0.015099</td>\n",
              "      <td>0.047084</td>\n",
              "      <td>7.19</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>118756.0</td>\n",
              "      <td>-0.634777</td>\n",
              "      <td>-1.605719</td>\n",
              "      <td>0.140014</td>\n",
              "      <td>-1.538825</td>\n",
              "      <td>-1.805853</td>\n",
              "      <td>0.699661</td>\n",
              "      <td>1.630540</td>\n",
              "      <td>-0.016779</td>\n",
              "      <td>-0.933695</td>\n",
              "      <td>-0.325517</td>\n",
              "      <td>0.302300</td>\n",
              "      <td>-0.312938</td>\n",
              "      <td>-0.285298</td>\n",
              "      <td>-0.336641</td>\n",
              "      <td>-1.468824</td>\n",
              "      <td>1.302678</td>\n",
              "      <td>-0.078134</td>\n",
              "      <td>-0.955677</td>\n",
              "      <td>0.941878</td>\n",
              "      <td>1.114855</td>\n",
              "      <td>0.199409</td>\n",
              "      <td>-0.372449</td>\n",
              "      <td>1.347100</td>\n",
              "      <td>-0.534399</td>\n",
              "      <td>-0.527257</td>\n",
              "      <td>-0.654911</td>\n",
              "      <td>-0.101362</td>\n",
              "      <td>0.074394</td>\n",
              "      <td>527.17</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Time        V1        V2        V3        V4        V5        V6  \\\n",
              "0  100902.0  1.908902 -0.669120 -0.114980  0.264529 -0.595843  0.603159   \n",
              "1  135812.0 -0.008638  0.131883  0.635344 -0.399317  0.577579  0.332326   \n",
              "2  159914.0  1.880701 -0.034917 -0.698435  1.484986 -0.299145 -1.002989   \n",
              "3  161123.0 -0.026897  1.515108 -1.569530 -0.104253  1.051111 -1.782796   \n",
              "4  118756.0 -0.634777 -1.605719  0.140014 -1.538825 -1.805853  0.699661   \n",
              "\n",
              "         V7        V8        V9       V10       V11       V12       V13  \\\n",
              "0 -1.203876  0.300718  2.606065 -0.213019  1.189008 -2.281706  0.650900   \n",
              "1  0.527800 -0.133700  0.232359  0.004998 -0.003635  0.619642  0.684039   \n",
              "2  0.147486 -0.279035  0.469875  0.193943 -0.465595  0.708619  0.340083   \n",
              "3  1.336510 -0.133662 -0.871216 -1.183370 -0.461643  0.235192  0.509155   \n",
              "4  1.630540 -0.016779 -0.933695 -0.325517  0.302300 -0.312938 -0.285298   \n",
              "\n",
              "        V14       V15       V16       V17       V18       V19       V20  \\\n",
              "0  1.514962 -0.411332  0.990916 -0.216454  1.041724 -0.114399 -0.187281   \n",
              "1 -0.323506 -0.354839  0.616552 -1.259022  0.500995  0.814613  0.048418   \n",
              "2  0.192576  0.076515 -0.277451 -0.309101 -0.435506 -0.673627 -0.182125   \n",
              "3 -0.281912  0.183666 -0.410558  0.888893  0.136806 -0.002083 -0.198616   \n",
              "4 -0.336641 -1.468824  1.302678 -0.078134 -0.955677  0.941878  1.114855   \n",
              "\n",
              "        V21       V22       V23       V24       V25       V26       V27  \\\n",
              "0 -0.090820 -0.048897  0.293753  0.154149 -0.638667  0.423484 -0.058530   \n",
              "1 -0.179809 -0.340650  0.142739 -1.047634 -1.118483 -0.115849 -0.163665   \n",
              "2  0.136601  0.515065  0.126878  0.399356  0.054687 -0.622530  0.021559   \n",
              "3  0.267003  0.795362 -0.206794 -0.059156 -0.098000 -0.149678 -0.015099   \n",
              "4  0.199409 -0.372449  1.347100 -0.534399 -0.527257 -0.654911 -0.101362   \n",
              "\n",
              "        V28  Amount  Fraud  Normal  \n",
              "0 -0.049864   39.00      0     1.0  \n",
              "1 -0.089465   65.91      0     1.0  \n",
              "2 -0.032091   46.47      0     1.0  \n",
              "3  0.047084    7.19      0     1.0  \n",
              "4  0.074394  527.17      0     1.0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kE75Y6II89Xo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "67e28af3-5af1-4429-cfd6-bf7b4db16975"
      },
      "source": [
        "print(df[df.Fraud==1][['Fraud', 'Normal']])"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        Fraud  Normal\n",
            "541         1     0.0\n",
            "623         1     0.0\n",
            "4920        1     0.0\n",
            "6108        1     0.0\n",
            "6329        1     0.0\n",
            "6331        1     0.0\n",
            "6334        1     0.0\n",
            "6336        1     0.0\n",
            "6338        1     0.0\n",
            "6427        1     0.0\n",
            "6446        1     0.0\n",
            "6472        1     0.0\n",
            "6529        1     0.0\n",
            "6609        1     0.0\n",
            "6641        1     0.0\n",
            "6717        1     0.0\n",
            "6719        1     0.0\n",
            "6734        1     0.0\n",
            "6774        1     0.0\n",
            "6820        1     0.0\n",
            "6870        1     0.0\n",
            "6882        1     0.0\n",
            "6899        1     0.0\n",
            "6903        1     0.0\n",
            "6971        1     0.0\n",
            "8296        1     0.0\n",
            "8312        1     0.0\n",
            "8335        1     0.0\n",
            "8615        1     0.0\n",
            "8617        1     0.0\n",
            "...       ...     ...\n",
            "251891      1     0.0\n",
            "251904      1     0.0\n",
            "252124      1     0.0\n",
            "252774      1     0.0\n",
            "254344      1     0.0\n",
            "254395      1     0.0\n",
            "255403      1     0.0\n",
            "255556      1     0.0\n",
            "258403      1     0.0\n",
            "261056      1     0.0\n",
            "261473      1     0.0\n",
            "261925      1     0.0\n",
            "262560      1     0.0\n",
            "262826      1     0.0\n",
            "263080      1     0.0\n",
            "263274      1     0.0\n",
            "263324      1     0.0\n",
            "263877      1     0.0\n",
            "268375      1     0.0\n",
            "272521      1     0.0\n",
            "274382      1     0.0\n",
            "274475      1     0.0\n",
            "275992      1     0.0\n",
            "276071      1     0.0\n",
            "276864      1     0.0\n",
            "279863      1     0.0\n",
            "280143      1     0.0\n",
            "280149      1     0.0\n",
            "281144      1     0.0\n",
            "281674      1     0.0\n",
            "\n",
            "[492 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLw01N8V89Xq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "543bea65-f103-462d-f9b4-cb13a29336fe"
      },
      "source": [
        "# Set X_train equal to 80% of the fraudulent transactions.\n",
        "FraudSample  = Fraud.sample(frac=0.8)\n",
        "NormalSample = Normal.sample(frac=0.8)\n",
        "count_Frauds = len(FraudSample)\n",
        "# Add 80% of the normal transactions to X_train.\n",
        "for_train = pd.concat([FraudSample, NormalSample], axis=0)\n",
        "\n",
        "# X_test contains all the transaction not in X_train.\n",
        "for_test = df.loc[~df.index.isin(for_train.index)]\n",
        "\n",
        "print('len(for_train)  : ',len(for_train))\n",
        "print('len(for_test)   : ',len(for_test))"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len(for_train)  :  227846\n",
            "len(for_test)   :  56961\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAA8gqbW89Xs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "9c3b2b3d-8c70-4b00-c9c3-85b2c55d0621"
      },
      "source": [
        "#Shuffle the dataframes so that the training is done in a random order.\n",
        "for_train = for_train.sample(frac=1).reset_index(drop=True)\n",
        "for_test = for_test.sample(frac=1).reset_index(drop=True)\n",
        "#for_test = np.random.shuffle(for_test)\n",
        "\n",
        "# Add our target features to y_train and y_test.\n",
        "X_train = for_train.drop(['Fraud', 'Normal'], axis = 1)\n",
        "# Drop target features from X_train and X_test.\n",
        "# Fraud, Normal 컬럼 drop\n",
        "y_train = for_train[['Fraud', 'Normal']]\n",
        "\n",
        "# Add our target features to y_train and y_test.\n",
        "X_test = for_test.drop(['Fraud', 'Normal'], axis = 1)\n",
        "# Drop target features from X_train and X_test.\n",
        "#  Fraud, Normal 컬럼 drop\n",
        "y_test = for_test[['Fraud', 'Normal']]\n",
        "\n",
        "#Check to ensure all of the training/testing dataframes are of the correct length\n",
        "print('len(X_train) : ',len(X_train))\n",
        "print('len(y_train) : ',len(y_train))\n",
        "print('len(X_test)  : ',len(X_test))\n",
        "print('len(y_test)  : ',len(y_test))"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len(X_train) :  227846\n",
            "len(y_train) :  227846\n",
            "len(X_test)  :  56961\n",
            "len(y_test)  :  56961\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToqrjzCXP-fa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "4060e018-5098-4ee2-a94f-ecbbcc616e61"
      },
      "source": [
        "y_train[0:5]"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Fraud</th>\n",
              "      <th>Normal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Fraud  Normal\n",
              "0      0     1.0\n",
              "1      0     1.0\n",
              "2      0     1.0\n",
              "3      0     1.0\n",
              "4      0     1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeMDChMz89Xu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "5f3e4fe9-73fe-4e0d-e939-31606f9412de"
      },
      "source": [
        "'''\n",
        "Due to the imbalance in the data, ratio will act as an equal weighting system for our model. \n",
        "By dividing the number of transactions by those that are fraudulent, ratio will equal the value that when multiplied\n",
        "by the number of fraudulent transactions will equal the number of normal transaction. \n",
        "Simply put: # of fraud * ratio = # of normal\n",
        "'''\n",
        "\n",
        "ratio = len(X_train) / count_Frauds\n",
        "print('ratio :', ratio)\n",
        "\n",
        "y_train.Fraud *= ratio*3\n",
        "y_test.Fraud *= ratio*3\n"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ratio : 578.2893401015228\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py:5096: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  self[name] = value\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJ8MnLr-89Xv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "54809fc4-a3f9-4ed0-8831-444f115aa2cf"
      },
      "source": [
        "columns_names = X_train.columns.values\n",
        "print('columns_names : ',columns_names)\n",
        "\n",
        "# Normalize\n",
        "for feature in columns_names:\n",
        "    mean, std = df[feature].mean(), df[feature].std()\n",
        "    # print('feature :',feature , 'mean : ', mean , 'std :', std)\n",
        "    X_train.loc[:, feature] = (X_train[feature] - mean) / std\n",
        "    X_test.loc[:, feature] = (X_test[feature] - mean) / std"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "columns_names :  ['Time' 'V1' 'V2' 'V3' 'V4' 'V5' 'V6' 'V7' 'V8' 'V9' 'V10' 'V11' 'V12'\n",
            " 'V13' 'V14' 'V15' 'V16' 'V17' 'V18' 'V19' 'V20' 'V21' 'V22' 'V23' 'V24'\n",
            " 'V25' 'V26' 'V27' 'V28' 'Amount']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rmQC5B089Xy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "outputId": "bd1c7bd7-8693-41f5-d18b-93ff45abd9a4"
      },
      "source": [
        "'''\n",
        "Train the Neural Net\n",
        "'''\n",
        "\n",
        "# Split the testing data into validation and testing sets\n",
        "split = int(len(y_test)/2)\n",
        "print('split : ', split)\n",
        "\n",
        "train_x = X_train.as_matrix()\n",
        "train_y = y_train.as_matrix()\n",
        "valid_x = X_test.as_matrix()[:split]\n",
        "valid_y = y_test.as_matrix()[:split]\n",
        "test_x = X_test.as_matrix()[split:]\n",
        "test_y = y_test.as_matrix()[split:]\n",
        "\n",
        "print('type(inputX)  : ', type(train_x))\n",
        "print('type(X_train) : ', type(X_train))\n",
        "\n",
        "print('y_train.Normal.value_counts() :',y_train.Normal.value_counts())\n",
        "print('y_train.Fraud.value_counts() :',y_train.Fraud.value_counts())\n",
        "\n",
        "print('y_test.Normal.value_counts() :',y_test.Normal.value_counts())\n",
        "print('y_test.Fraud.value_counts() :',y_test.Fraud.value_counts())\n",
        "\n",
        "print('C valid_y', np.where(valid_y[:, 0] > 0, 1, 0).sum())\n",
        "print('C test_y ', np.where(test_y[:, 0] > 0, 1, 0).sum())\n",
        "\n",
        "print('inputX :',train_x.shape)\n",
        "print('inputY :',train_y.shape)\n",
        "print('valid_x :',valid_x.shape)\n",
        "print('valid_y :',valid_y.shape)\n",
        "print('test_x :',test_x.shape)\n",
        "print('test_y :',test_y.shape)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "split :  28480\n",
            "type(inputX)  :  <class 'numpy.ndarray'>\n",
            "type(X_train) :  <class 'pandas.core.frame.DataFrame'>\n",
            "y_train.Normal.value_counts() : 1.0    227452\n",
            "0.0       394\n",
            "Name: Normal, dtype: int64\n",
            "y_train.Fraud.value_counts() : 0.00000       227452\n",
            "1734.86802       394\n",
            "Name: Fraud, dtype: int64\n",
            "y_test.Normal.value_counts() : 1.0    56863\n",
            "0.0       98\n",
            "Name: Normal, dtype: int64\n",
            "y_test.Fraud.value_counts() : 0.00000       56863\n",
            "1734.86802       98\n",
            "Name: Fraud, dtype: int64\n",
            "C valid_y 46\n",
            "C test_y  52\n",
            "inputX : (227846, 30)\n",
            "inputY : (227846, 2)\n",
            "valid_x : (28480, 30)\n",
            "valid_y : (28480, 2)\n",
            "test_x : (28481, 30)\n",
            "test_y : (28481, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wYwyOIf3bsT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Confusion Matrix\n",
        "def get_conf_rate(model, testX, testY):\n",
        "    # 모델 예측\n",
        "    predicted_y = model.predict(testX)\n",
        "\n",
        "    # compare predicted_y & testY\n",
        "    conf_cnt = collections.defaultdict(int)\n",
        "    for pr_y, real_y in zip(predicted_y, testY):\n",
        "        conf = np.argmax(pr_y), np.argmax(real_y)\n",
        "        conf_cnt[conf] += 1\n",
        "\n",
        "    # 0번방 == Fraud, 1번방 == Normal\n",
        "    TP = conf_cnt[(0 ,0)]\n",
        "    FN = conf_cnt[(1, 0)]\n",
        "    TN = conf_cnt[(1, 1)]\n",
        "    FP = conf_cnt[(0, 1)]\n",
        "\n",
        "    Acc = (TP + TN) / (TP + TN + FP + FN)\n",
        "    try:\n",
        "        Precision = TP / (TP + FP)\n",
        "    except: # 학습초기에 decision by zero 발생 가능 (범죄라고 판단한 건수가 zero인 상황\n",
        "        Precision = 0\n",
        "      \n",
        "    try:\n",
        "        Recall = TP / (TP + FN)\n",
        "    except:\n",
        "        Recall = 0\n",
        "        raise\n",
        "\n",
        "    print('\\nTP :', TP, ' FN :',FN , ' TN :', TN, ' FP :', FP)\n",
        "    print('Recall: %.3f, Precision: %.3f, Acc: %.3f' % (Recall, Precision, Acc))\n",
        "    return Recall, Precision, Acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liBq7K_v89X2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Callback Class\n",
        "class mykerasCB(keras.callbacks.Callback):\n",
        "    def __init__(self):\n",
        "        super(mykerasCB, self).__init__() # 부모 클래스의 constructor 호출\n",
        "        self.hist = [] # confusion matrix값 (비율)을 기록하기 위한 것\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "        if epoch % 10 == 9: # 10번 epoch 마다 출력해보기\n",
        "          confusion = get_conf_rate(self.model, test_x, test_y)\n",
        "          self.hist.append(confusion)\n",
        "    def on_batch_end(self, batch, logs):\n",
        "        pass\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTBIZAfn89X4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "34f33f1d-a3f8-4ae5-9ee5-cc4d39115dd3"
      },
      "source": [
        "# Number of input nodes.\n",
        "input_nodes = train_x.shape[1]\n",
        "\n",
        "# Multiplier maintains a fixed ratio of nodes between each layer.\n",
        "mulitplier = 1.5\n",
        "\n",
        "# Number of nodes in each hidden layer\n",
        "hidden_nodes1 = 18\n",
        "hidden_nodes2 = round(hidden_nodes1 * mulitplier)\n",
        "hidden_nodes3 = round(hidden_nodes2 * mulitplier)\n",
        "print('노드수', hidden_nodes1, hidden_nodes2, hidden_nodes3)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "노드수 18 27 40\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bT-RauRF89X6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "50b1b9b7-30ec-485d-c374-b4e2f65ea8cf"
      },
      "source": [
        "print('======================  Keras   ======================')\n",
        "# 모델 구성하기\n",
        "model = Sequential()\n",
        "model.add(Dense(hidden_nodes1, input_dim=input_nodes, activation='sigmoid'))\n",
        "model.add(Dense(hidden_nodes2, activation='tanh'))\n",
        "model.add(Dense(hidden_nodes3, activation='sigmoid'))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "# Callback Class 선언\n",
        "cb = mykerasCB()\n",
        "\n",
        "# 모델 학습과정 설정하기\n",
        "model.compile(optimizer='Adam', loss='mean_squared_error', metrics=['accuracy'])"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "======================  Keras   ======================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAG2lpNN89X-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c1a417e9-9983-4506-b4b2-3fd8004ee2b9"
      },
      "source": [
        "# 모델 학습시키기\n",
        "hist = model.fit(train_x, train_y, epochs=50, batch_size=1024,\n",
        "                 callbacks = [cb], verbose=1)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "227846/227846 [==============================] - 1s 4us/step - loss: 2599.4386 - acc: 0.9419\n",
            "Epoch 2/50\n",
            "227846/227846 [==============================] - 1s 4us/step - loss: 2599.4231 - acc: 0.9435\n",
            "Epoch 3/50\n",
            "227846/227846 [==============================] - 1s 4us/step - loss: 2599.4277 - acc: 0.9412\n",
            "Epoch 4/50\n",
            "227846/227846 [==============================] - 1s 4us/step - loss: 2599.4197 - acc: 0.9505\n",
            "Epoch 5/50\n",
            "227846/227846 [==============================] - 1s 4us/step - loss: 2599.4177 - acc: 0.9482\n",
            "Epoch 6/50\n",
            "227846/227846 [==============================] - 1s 4us/step - loss: 2599.4249 - acc: 0.9392\n",
            "Epoch 7/50\n",
            "227846/227846 [==============================] - 1s 4us/step - loss: 2599.4366 - acc: 0.9341\n",
            "Epoch 8/50\n",
            "227846/227846 [==============================] - 1s 4us/step - loss: 2599.4170 - acc: 0.9475\n",
            "Epoch 9/50\n",
            "227846/227846 [==============================] - 1s 4us/step - loss: 2599.4132 - acc: 0.9512\n",
            "Epoch 10/50\n",
            "227846/227846 [==============================] - 1s 4us/step - loss: 2599.4120 - acc: 0.9523\n",
            "\n",
            "TP : 50  FN : 1551  TN : 26878  FP : 2\n",
            "Recall: 0.031, Precision: 0.962, Acc: 0.945\n",
            "Epoch 11/50\n",
            "227846/227846 [==============================] - 1s 4us/step - loss: 2599.4213 - acc: 0.9430\n",
            "Epoch 12/50\n",
            "227846/227846 [==============================] - 1s 4us/step - loss: 2599.4276 - acc: 0.9482\n",
            "Epoch 13/50\n",
            "227846/227846 [==============================] - 1s 4us/step - loss: 2599.4204 - acc: 0.9434\n",
            "Epoch 14/50\n",
            "227846/227846 [==============================] - 1s 4us/step - loss: 2599.4261 - acc: 0.9457\n",
            "Epoch 15/50\n",
            "227846/227846 [==============================] - 1s 4us/step - loss: 2599.4250 - acc: 0.9422\n",
            "Epoch 16/50\n",
            "227846/227846 [==============================] - 1s 4us/step - loss: 2599.4189 - acc: 0.9519\n",
            "Epoch 17/50\n",
            "227846/227846 [==============================] - 1s 4us/step - loss: 2599.4145 - acc: 0.9595\n",
            "Epoch 18/50\n",
            "227846/227846 [==============================] - 1s 4us/step - loss: 2599.4154 - acc: 0.9484\n",
            "Epoch 19/50\n",
            "227846/227846 [==============================] - 1s 4us/step - loss: 2599.4262 - acc: 0.9415\n",
            "Epoch 20/50\n",
            "227846/227846 [==============================] - 1s 4us/step - loss: 2599.4222 - acc: 0.9438\n",
            "\n",
            "TP : 50  FN : 1471  TN : 26958  FP : 2\n",
            "Recall: 0.033, Precision: 0.962, Acc: 0.948\n",
            "Epoch 21/50\n",
            "227846/227846 [==============================] - 1s 4us/step - loss: 2599.4180 - acc: 0.9457\n",
            "Epoch 22/50\n",
            "227846/227846 [==============================] - 1s 4us/step - loss: 2599.4086 - acc: 0.9592\n",
            "Epoch 23/50\n",
            "227846/227846 [==============================] - 1s 4us/step - loss: 2599.4086 - acc: 0.9555\n",
            "Epoch 24/50\n",
            "227846/227846 [==============================] - 1s 4us/step - loss: 2599.4057 - acc: 0.9583\n",
            "Epoch 25/50\n",
            "227846/227846 [==============================] - 1s 4us/step - loss: 2599.4064 - acc: 0.9584\n",
            "Epoch 26/50\n",
            "227846/227846 [==============================] - 1s 4us/step - loss: 2599.4263 - acc: 0.9426\n",
            "Epoch 27/50\n",
            "227846/227846 [==============================] - 1s 4us/step - loss: 2599.4179 - acc: 0.9490\n",
            "Epoch 28/50\n",
            "227846/227846 [==============================] - 1s 4us/step - loss: 2599.4072 - acc: 0.9555\n",
            "Epoch 29/50\n",
            "227846/227846 [==============================] - 1s 4us/step - loss: 2599.4068 - acc: 0.9593\n",
            "Epoch 30/50\n",
            "227846/227846 [==============================] - 1s 4us/step - loss: 2599.4032 - acc: 0.9606\n",
            "\n",
            "TP : 50  FN : 967  TN : 27462  FP : 2\n",
            "Recall: 0.049, Precision: 0.962, Acc: 0.966\n",
            "Epoch 31/50\n",
            "227846/227846 [==============================] - 1s 4us/step - loss: 2599.4015 - acc: 0.9626\n",
            "Epoch 32/50\n",
            "227846/227846 [==============================] - 1s 4us/step - loss: 2599.4100 - acc: 0.9555\n",
            "Epoch 33/50\n",
            "227846/227846 [==============================] - 1s 4us/step - loss: 2599.4139 - acc: 0.9525\n",
            "Epoch 34/50\n",
            "227846/227846 [==============================] - 1s 4us/step - loss: 2599.4030 - acc: 0.9616\n",
            "Epoch 35/50\n",
            "227846/227846 [==============================] - 1s 4us/step - loss: 2599.4234 - acc: 0.9456\n",
            "Epoch 36/50\n",
            "227846/227846 [==============================] - 1s 4us/step - loss: 2599.4121 - acc: 0.9524\n",
            "Epoch 37/50\n",
            "227846/227846 [==============================] - 1s 4us/step - loss: 2599.4098 - acc: 0.9548\n",
            "Epoch 38/50\n",
            "227846/227846 [==============================] - 1s 4us/step - loss: 2599.4077 - acc: 0.9572\n",
            "Epoch 39/50\n",
            "227846/227846 [==============================] - 1s 4us/step - loss: 2599.4048 - acc: 0.9595\n",
            "Epoch 40/50\n",
            "227846/227846 [==============================] - 1s 4us/step - loss: 2599.4047 - acc: 0.9596\n",
            "\n",
            "TP : 50  FN : 1123  TN : 27306  FP : 2\n",
            "Recall: 0.043, Precision: 0.962, Acc: 0.960\n",
            "Epoch 41/50\n",
            "227846/227846 [==============================] - 1s 4us/step - loss: 2599.4109 - acc: 0.9546\n",
            "Epoch 42/50\n",
            "227846/227846 [==============================] - 1s 4us/step - loss: 2599.4050 - acc: 0.9608\n",
            "Epoch 43/50\n",
            "227846/227846 [==============================] - 1s 4us/step - loss: 2599.3997 - acc: 0.9652\n",
            "Epoch 44/50\n",
            "227846/227846 [==============================] - 1s 4us/step - loss: 2599.4168 - acc: 0.9487\n",
            "Epoch 45/50\n",
            "227846/227846 [==============================] - 1s 4us/step - loss: 2599.4100 - acc: 0.9531\n",
            "Epoch 46/50\n",
            "227846/227846 [==============================] - 1s 4us/step - loss: 2599.4075 - acc: 0.9644\n",
            "Epoch 47/50\n",
            "227846/227846 [==============================] - 1s 4us/step - loss: 2599.4106 - acc: 0.9543\n",
            "Epoch 48/50\n",
            "227846/227846 [==============================] - 1s 4us/step - loss: 2599.3990 - acc: 0.9641\n",
            "Epoch 49/50\n",
            "227846/227846 [==============================] - 1s 4us/step - loss: 2599.4016 - acc: 0.9647\n",
            "Epoch 50/50\n",
            "227846/227846 [==============================] - 1s 4us/step - loss: 2599.4075 - acc: 0.9605\n",
            "\n",
            "TP : 49  FN : 948  TN : 27481  FP : 3\n",
            "Recall: 0.049, Precision: 0.942, Acc: 0.967\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KX8LQ41T89X_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "a568dc49-84e7-4423-e180-14addf079242"
      },
      "source": [
        "# 최종 confusion matrix 구하기\n",
        "get_conf_rate(model, test_x, test_y)\n",
        "\n",
        "# 모델 평가하기\n",
        "loss_and_metrics = model.evaluate(valid_x, valid_y)\n",
        "print('loss_and_metrics : ' + str(loss_and_metrics))"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "TP : 50  FN : 2  TN : 26995  FP : 1434\n",
            "Recall: 0.962, Precision: 0.034, Acc: 0.950\n",
            "28480/28480 [==============================] - 1s 21us/step\n",
            "loss_and_metrics : [2428.0114349107835, 0.9457865168539326]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNeHisqb9Tom",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "16dcc835-0805-4f3c-f833-052f46e2ae3f"
      },
      "source": [
        "# Recall, Precision, Acc\n",
        "fig, ax =  plt.subplots()\n",
        "\n",
        "acc = [h[2] for h in cb.hist]\n",
        "pre = [h[1] for h in cb.hist]\n",
        "recall = [h[0] for h in cb.hist]\n",
        "\n",
        "ax.plot(acc, label='acc')\n",
        "ax.plot(pre, label='pre')\n",
        "ax.plot(recall, label='recall')\n",
        "\n",
        "ax.set_xlabel('epochs(x10)')\n",
        "ax.legend()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print('======================================================')\n",
        "print('======================  Keras   ======================')\n",
        "print('======================================================')"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEKCAYAAADpfBXhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "======================================================\n",
            "======================  Keras   ======================\n",
            "======================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYv-MwSW6WEl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}